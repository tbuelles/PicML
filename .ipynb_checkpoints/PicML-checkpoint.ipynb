{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6f058e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame, Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0243812e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../Polygon/PolygonML.py\n",
    "'''Process Polygon data, select investigations to run, perform ML'''\n",
    "'''Code comprised of 4 cells: \n",
    "    ~ Cell 1 --> Choose investigation parameters\n",
    "    ~ Cell 2 --> Import libraries & data\n",
    "    ~ Cell 3 --> Define the ML function\n",
    "    ~ Cell 4 --> Iterate through selected investigations performing the ML\n",
    "    To run: Select the investigations to consider in cell 1, run cell 1, run cell 2, run cell 3, run cell 4,\n",
    "            ...then can repeatedly edit investigations in cell 1 then running cells 1 & 4 sequentially to perform different ML investigations.\n",
    "    notes:  ensure path to datafile is correct on line 33, there is functionality for padding (although not used in investigations as performed worse),\n",
    "    ...can input padding choice into function directly in cell 4, additional printing options at end of ML function and end of cell 4 which can be uncommented.\n",
    "'''\n",
    "#Select investigations to run\n",
    "input_set = [1,2]       #...select 1 --> vertices, or 2 --> pluckers, or both\n",
    "output_set = [7,8,9,10] #...select 7 --> volume, 8 --> dual volume, 9 --> gorenstein index, 10 --> codimension, or any combination thereof\n",
    "number_vertex_set = [3,4,5,6] #...select which datasets of polygons with this many vertices to consider, note including '0' will run ML on all polygons using vector padding (can edit padding functionality in function call: what choice of number to pad with)\n",
    "crossval_check = True   #...select whether to perform cross-validation, default is 5-fold (can edit in function directly)\n",
    "tt_ratio_set = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]    #...if running over varying train/test split ratios (where no cross-validation, st k=1) select the train set proportions to consider from the interval (0,1)\n",
    "gcd_scheme = 0          #...select which gcd augmentaion scheme to use: 0 --> none, 1 --> pairwise gcds, 2 --> (n-1)-gcds\n",
    "inversion_check = False #...choose whether to run the inversion investigation, swapping the final vector entry with the learned parameter\n",
    "\n",
    "#%%\n",
    "#Import libraries\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "from math import floor\n",
    "from itertools import chain, combinations\n",
    "from copy import deepcopy\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "\n",
    "#Import Polygon Data\n",
    "with sqlite3.connect(\"./dim_2_plucker.db\") as db: #...ensure this is the correct path to the datafile\n",
    "    c = db.cursor()\n",
    "    df = pd.read_sql_query(\"SELECT * FROM dim_2_plucker\", db)   #...read database into a pandas dataframe\n",
    "    headings = df.columns.values                                #...save the headings of each column in the table\n",
    "    data = df.values                                            #...convert pandas dataframe to np.array\n",
    "del(c,df)\n",
    "#Reformat dual volume data to floats\n",
    "for polygon in data:\n",
    "    if isinstance(polygon[8],str): #...where dual volume interpreted as string convert to a float\n",
    "        polygon[8] = float(polygon[8].split('/')[0])/float(polygon[8].split('/')[1])\n",
    "del(polygon)\n",
    "\n",
    "#Extract the ranges of the polygon parameters\n",
    "Y_Ranges = [[min([poly[i] for poly in data]),max([poly[i] for poly in data])] for i in [3,4,5,6,7,8,9,10]]\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "#%% #ML data - set-up general function for all investigations\n",
    "def ML(X=2,Y=7,poly_n=0,Pad=1,Pchoice=0,gcd=0,k_cv=5,split=0.8):\n",
    "    #Extract relevant parts of data to ML \n",
    "    #Data selection hyper-params\n",
    "    X_choice, Y_choice = X, Y   #...choose what to ML (use 'headings': id, vertices, plucker, plucker_len, num_vertices, num_points, num_interior_points, volume, dual_volume, gorenstein_idx, codimension)\n",
    "    n = poly_n                  #...select number of vertices to ML, use '0' to mean all\n",
    "    Pad_check = Pad             #...true to perform padding, false to select according to length\n",
    "    Pad_choice = Pchoice        #...number to pad onto end of vectors, only relevant when padding\n",
    "    GCD_scheme = gcd             #...whether to augment plucker coords by: (0) nothing, (1) pairwise gcds, (2) (n-1)-gcds\n",
    "    Y_choice_range = Y_Ranges[Y_choice-3][1] - Y_Ranges[Y_choice-3][0] #... (max - min) for the selected varible to ML\n",
    "\n",
    "    #Extract relevant X & Y data\n",
    "    polygons_X, polygons_Y, last_poly_pts = [], [], []\n",
    "    for idx, poly in enumerate(data):\n",
    "        if int(poly[4]) == n or n == 0: #...extract only polygons with n vertices, or all polygons if n==0 (inefficient extra 'ifs' but this section is not the time bottleneck)\n",
    "            if X_choice == 1 and poly[1]!=last_poly_pts: #...skip repeated lines in dataset (where plucker coords permuted)\n",
    "                last_poly_pts = poly[1] #...keep track of last polygon, so know when moved onto next one\n",
    "                polygons_X.append(list(chain(*literal_eval(poly[X_choice])))) #...if using vertices need to flatten to a vector                    \n",
    "                polygons_Y.append(poly[Y_choice])\n",
    "            elif X_choice == 2: \n",
    "                if GCD_scheme == 1:   #...augment vectors with pairwise gcds\n",
    "                    polygons_X.append(literal_eval(poly[X_choice])+[np.gcd(*np.absolute(x)) for x in combinations(literal_eval(poly[X_choice]),2)])\n",
    "                elif GCD_scheme == 2: #...augment vectors with (n-1)-gcds\n",
    "                    polygons_X.append(literal_eval(poly[X_choice])+[np.gcd.reduce(np.absolute(x)) for x in combinations(literal_eval(poly[2]),poly[3]-1)])\n",
    "                else: polygons_X.append(literal_eval(poly[X_choice]))\n",
    "                polygons_Y.append(poly[Y_choice])\n",
    "    number_polygon = len(polygons_Y)\n",
    "    \n",
    "    #Run inversion, editing data accordingly swapping param with last entry of the input vector\n",
    "    if inversion_check:\n",
    "        params = deepcopy(polygons_Y)\n",
    "        for poly in range(len(polygons_X)):\n",
    "            polygons_Y[poly] = polygons_X[poly][-1]\n",
    "            polygons_X[poly][-1] = params[poly]\n",
    "        Y_choice_range = max(polygons_Y)-min(polygons_Y)\n",
    "        del(params)\n",
    "\n",
    "    #Pad plucker coordinates if desired (only needed for n == 0)\n",
    "    if Pad_check and n != 0:\n",
    "        max_length = max(map(len,polygons_X))\n",
    "        for polygon in polygons_X:\n",
    "            while len(polygon) < max_length: #...pad all X vectors to the maximum length\n",
    "                polygon += [Pad_choice]\n",
    "        del(polygon,poly)\n",
    "\n",
    "    #k-fold cross-val data setup\n",
    "    k = k_cv #...number of cross-validations to perform\n",
    "    tt_split = split #... only relevant if no cross-validation, sets the size of the train:test ratio split\n",
    "    ML_data = [[polygons_X[index],polygons_Y[index]] for index in range(number_polygon)]\n",
    "\n",
    "    np.random.shuffle(ML_data)\n",
    "    Training_data, Training_values, Testing_data, Testing_values = [], [], [], []\n",
    "    if k > 1:\n",
    "        s = int(floor(len(ML_data)/k))        #...number of datapoints in each validation split\n",
    "        for i in range(k):\n",
    "            Training_data.append([HS[0] for HS in ML_data[:i*s]]+[HS[0] for HS in ML_data[(i+1)*s:]])\n",
    "            Training_values.append([HS[1] for HS in ML_data[:i*s]]+[HS[1] for HS in ML_data[(i+1)*s:]])\n",
    "            Testing_data.append([HS[0] for HS in ML_data[i*s:(i+1)*s]])\n",
    "            Testing_values.append([HS[1] for HS in ML_data[i*s:(i+1)*s]])\n",
    "    elif k == 1:\n",
    "        s = int(floor(len(ML_data)*tt_split)) #...number of datapoints in train split\n",
    "        Training_data.append([HS[0] for HS in ML_data[:s]])\n",
    "        Training_values.append([HS[1] for HS in ML_data[:s]])\n",
    "        Testing_data.append([HS[0] for HS in ML_data[s:]])\n",
    "        Testing_values.append([HS[1] for HS in ML_data[s:]])\n",
    "\n",
    "    #Create and Train NN\n",
    "    #Define NN hyper-parameters\n",
    "    def act_fn(x): return keras.activations.relu(x,alpha=0.01) #...leaky-ReLU activation\n",
    "    number_of_epochs = 20           #...number of times to run training data through NN\n",
    "    size_of_batches = 32            #...number of datapoints the NN sees per iteration of optimiser (high batch means more accurate param updating, but less frequently) \n",
    "    layer_sizes = [64,64,64,64]     #...number and size of the dense NN layers\n",
    "\n",
    "    #Define lists to record training history and learning measures\n",
    "    hist_data = []               #...training data (output of .fit(), used for plotting)\n",
    "    metric_loss_data = []        #...list of learning measure losses [MAE,logcosh,MAPE,MSE]\n",
    "    acc_list = []                #...list of accuracy ranges [\\pm 0.5, \\pm 5%, \\pm 10%]\n",
    "\n",
    "    #Train k independent NNs for k-fold cross-validation (learning measures then averaged over)\n",
    "    for i in range(k):\n",
    "        #Setup NN\n",
    "        model = keras.Sequential()\n",
    "        for layer_size in layer_sizes:\n",
    "            model.add(keras.layers.Dense(layer_size, activation=act_fn))\n",
    "            #model.add(keras.layers.Dropout(0.1)) #...dropout layer to reduce chance of overfitting to training data\n",
    "        model.add(keras.layers.Dense(1))\n",
    "        model.compile(optimizer='adam', loss='logcosh') #...choose from: [MAE,MAPE,MSE,logcosh]\n",
    "        #Train NN\n",
    "        hist_data.append(model.fit(Training_data[i], Training_values[i], batch_size=size_of_batches, epochs=number_of_epochs, shuffle=True, validation_split=0., verbose=0))\n",
    "        #Test NN\n",
    "        predictions = np.ndarray.flatten(model.predict(Testing_data[i]))\n",
    "        metric_loss_data.append([float(keras.losses.MAE(Testing_values[i],predictions)),float(keras.losses.logcosh(Testing_values[i],predictions)),float(keras.losses.MAPE(Testing_values[i],predictions)),float(keras.losses.MSE(Testing_values[i],predictions))])\n",
    "        count_A, count_B, count_C = 0, 0, 0\n",
    "        for test in range(len(predictions)):\n",
    "            if Testing_values[i][test]-0.5 <= predictions[test] <= Testing_values[i][test]+0.5:\n",
    "                count_A += 1\n",
    "                count_B += 1\n",
    "                count_C += 1\n",
    "            elif Testing_values[i][test]-Y_choice_range*0.025 <= predictions[test] <= Testing_values[i][test]+Y_choice_range*0.025:\n",
    "                count_B += 1\n",
    "                count_C += 1\n",
    "            elif Testing_values[i][test]-Y_choice_range*0.05 <= predictions[test] <= Testing_values[i][test]+Y_choice_range*0.05:\n",
    "                count_C += 1\n",
    "        acc_list.append([count_A/len(predictions),count_B/len(predictions),count_C/len(predictions)])\n",
    "\n",
    "    #Output averaged testing metric accuracies and losses\n",
    "    with open('./MLResults.txt','a') as myfile:\n",
    "        myfile.write('Accuracies [\\pm 0.5, \\pm 0.025*range, \\pm 0.05*range]: '+str(np.sum(acc_list,axis=0)/k)+'\\nLosses [MAE, Log(cosh), MAPE, MSE]: '+str(np.sum(metric_loss_data,axis=0)/k))\n",
    "    #print('\\n####################################') #...uncomment to print measures as the investigations are running\n",
    "    #print('Hyper-params:',headings[X_choice],'-->',headings[Y_choice],', with number vertices:',n,', for dataset size:',number_polygon,'polygons, with...',Pad_check,Pad_choice,Selection_len)\n",
    "    #print('Average measures:')\n",
    "    #print('[MAE, Log(cosh), MAPE, MSE]:',np.sum(metric_loss_data,axis=0)/k)\n",
    "    #print('Accuracies [\\pm 0.5, \\pm 0.025*range, \\pm 0.05*range]:',np.sum(acc_list,axis=0)/k)\n",
    "\n",
    "    return [[X_choice,Y_choice,n],np.sum(acc_list,axis=0)/k,np.sum(metric_loss_data,axis=0)/k] #[[X_choice,Y_choice,n],Testing_values[-1],list(predictions)] #...optionally output true & predicted values for plotting\n",
    "\n",
    "###############################################################################\n",
    "#%% #Set-up file to write results to\n",
    "with open('./MLResults.txt','w') as myfile:\n",
    "    myfile.write('ML Results for Vert/Pluck --> Params, for n 3->6 in Dense LeakyReLU NN 4x64')\n",
    "\n",
    "#Run ML for variety of investigation hyper-params\n",
    "ML_results = [] #...save all investigation information\n",
    "for x in input_set: \n",
    "    for y in output_set: \n",
    "        with open('./MLResults.txt','a') as myfile:\n",
    "            myfile.write('\\n\\n#####################\\nHyper-params: '+str(headings[x])+' --> '+str(headings[y]))\n",
    "        for n in number_vertex_set:\n",
    "            with open('./MLResults.txt','a') as myfile:\n",
    "                myfile.write('\\nn = '+str(n)+': ')\n",
    "            if crossval_check:\n",
    "                ML_results.append(ML(x,y,n,gcd=gcd_scheme))\n",
    "            else:\n",
    "                for tt in tt_ratio_set:\n",
    "                    with open('./MLResults.txt','a') as myfile:\n",
    "                        myfile.write('\\nTrain proportion = '+str(tt)+': ')\n",
    "                    ML_results.append(ML(x,y,n,gcd=gcd_scheme,k_cv=1,split=tt))\n",
    "\n",
    "#Output the results in a raw format for data processing if desired\n",
    "with open('./RawOutput.txt','w') as myfile2:\n",
    "    myfile2.write(str(ML_results))\n",
    "\n",
    "#print(ML_results) #...uncomment if wish to see all investigation information printed to current terminal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6fcc8f1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{{1, 0, 0, -1}, {0, 1, 0, -1}, {0, 0, 1, -1}}</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{{1, 1, 1, -3}, {0, 4, 0, -4}, {0, 0, 4, -4}}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{{1, 0, 0, -2}, {0, 1, 0, -2}, {0, 0, 1, -1}}</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{{1, 1, 1, -5}, {0, 3, 0, -6}, {0, 0, 3, -3}}</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{{1, 0, 0, -4}, {0, 1, 0, -4}, {0, 0, 1, -3}}</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4314</th>\n",
       "      <td>{{1, 0, 0, 1, 0, 1, -1, 0, -1, 0, -1, -1}, {0,...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4315</th>\n",
       "      <td>{{1, 0, 0, 1, 1, -1, -1, 0, -1, 0, 1, 0}, {0, ...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4316</th>\n",
       "      <td>{{1, 0, 1, 0, 0, 0, 0, -1, 0, -1, -1, -1}, {0,...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4317</th>\n",
       "      <td>{{1, 0, 0, -1, 1, 1, -1, -1, -1, 1, 0, 0, 0}, ...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4318</th>\n",
       "      <td>{{1, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1}...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4319 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      0   1\n",
       "0         {{1, 0, 0, -1}, {0, 1, 0, -1}, {0, 0, 1, -1}}  19\n",
       "1         {{1, 1, 1, -3}, {0, 4, 0, -4}, {0, 0, 4, -4}}   1\n",
       "2         {{1, 0, 0, -2}, {0, 1, 0, -2}, {0, 0, 1, -1}}  18\n",
       "3         {{1, 1, 1, -5}, {0, 3, 0, -6}, {0, 0, 3, -3}}   4\n",
       "4         {{1, 0, 0, -4}, {0, 1, 0, -4}, {0, 0, 1, -3}}  16\n",
       "...                                                 ...  ..\n",
       "4314  {{1, 0, 0, 1, 0, 1, -1, 0, -1, 0, -1, -1}, {0,...   9\n",
       "4315  {{1, 0, 0, 1, 1, -1, -1, 0, -1, 0, 1, 0}, {0, ...  11\n",
       "4316  {{1, 0, 1, 0, 0, 0, 0, -1, 0, -1, -1, -1}, {0,...  10\n",
       "4317  {{1, 0, 0, -1, 1, 1, -1, -1, -1, 1, 0, 0, 0}, ...  10\n",
       "4318  {{1, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1}...  10\n",
       "\n",
       "[4319 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Pic4319.csv\", header=None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f5ec9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
